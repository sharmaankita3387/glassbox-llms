{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47397bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "!pip install manim\n",
    "!pip install transformers torch matplotlib seaborn\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0509b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    attentions = outputs.attentions\n",
    "    return tokens, attentions, inputs[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name, output_attentions=True)\n",
    "model = AutoModel.from_pretrained(model_name, config=config)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cat that I love sat on the mat.\"\n",
    "tokens, attentions, input_ids = extract_attention(text, model, tokenizer)\n",
    "\n",
    "# Choose layer/head you want to animate\n",
    "layer_idx = 2\n",
    "head_idx = 5\n",
    "matrix = attentions[layer_idx][0, head_idx].cpu().numpy()\n",
    "\n",
    "# Save for Manim\n",
    "np.savez(\"attention_data.npz\", tokens=np.array(tokens), matrix=matrix,\n",
    "         layer_idx=layer_idx, head_idx=head_idx, text=text)\n",
    "print(\"Saved data for Manim.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc07f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(matrix, xticklabels=tokens, yticklabels=tokens,\n",
    "            cmap='Blues', annot=True, fmt='.2f')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title(f\"Layer {layer_idx}, Head {head_idx}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -ql -v WARNING AttentionHeadScene\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "\n",
    "class AttentionHeadScene(Scene):\n",
    "    def construct(self):\n",
    "        data = np.load(\"attention_data.npz\", allow_pickle=True)\n",
    "\n",
    "        # Convert NumPy strings → Python strings\n",
    "        raw_tokens = data[\"tokens\"]\n",
    "        tokens = [str(t) for t in raw_tokens.tolist()]\n",
    "        matrix = data[\"matrix\"]\n",
    "        layer_idx = int(data.get(\"layer_idx\", -1))\n",
    "        head_idx = int(data.get(\"head_idx\", -1))\n",
    "        text = str(data.get(\"text\", \"\"))\n",
    "\n",
    "        seq_len = len(tokens)\n",
    "\n",
    "        # --- Title ----------------------------------------------------------\n",
    "        title = Text(f\"Layer {layer_idx}, Head {head_idx}\", font_size=36)\n",
    "        title.to_edge(UP)\n",
    "        self.add(title)\n",
    "\n",
    "        # --- Token labels (top & left) -------------------------------------\n",
    "        col_labels = VGroup()\n",
    "        row_labels = VGroup()\n",
    "        cell_size = 0.6  # shrink a bit vs before\n",
    "\n",
    "        for j, tok in enumerate(tokens):\n",
    "            t = Text(tok, font_size=20)\n",
    "            t.next_to(ORIGIN, UP, buff=0.3)\n",
    "            t.shift(RIGHT * (j - (seq_len - 1)/2) * (cell_size + 0.05) + DOWN*0.5)\n",
    "            col_labels.add(t)\n",
    "\n",
    "        for i, tok in enumerate(tokens):\n",
    "            t = Text(tok, font_size=20)\n",
    "            # Align left of grid\n",
    "            t.shift(\n",
    "                LEFT * ((seq_len/2) * (cell_size + 0.05) + 1.2)  # left of grid\n",
    "                + DOWN * (i - (seq_len - 1)/2) * (cell_size + 0.05)\n",
    "            )\n",
    "            row_labels.add(t)\n",
    "\n",
    "        self.add(col_labels, row_labels)\n",
    "\n",
    "        # --- Heatmap cells with numbers ------------------------------------\n",
    "        cells = VGroup()\n",
    "        min_val = 0.0\n",
    "        max_val = 1.0  # attention is already in [0,1]\n",
    "\n",
    "        def value_to_color(v):\n",
    "            # 0 → light, 1 → dark blue\n",
    "            alpha = (v - min_val) / (max_val - min_val + 1e-8)\n",
    "            return interpolate_color(WHITE, BLUE, alpha)\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                v = float(matrix[i, j])\n",
    "\n",
    "                # cell position (grid centered around origin)\n",
    "                x = (j - (seq_len - 1)/2) * (cell_size + 0.05)\n",
    "                y = (-(i - (seq_len - 1)/2)) * (cell_size + 0.05)\n",
    "                pos = np.array([x, y, 0])\n",
    "\n",
    "                sq = Square(side_length=cell_size)\n",
    "                sq.set_fill(value_to_color(v), opacity=0.9)\n",
    "                sq.set_stroke(color=BLACK, width=0.5)\n",
    "                sq.move_to(pos)\n",
    "\n",
    "                # numeric label\n",
    "                num = Text(f\"{v:.2f}\", font_size=18)\n",
    "                num.move_to(pos)\n",
    "\n",
    "                # choose text color for contrast\n",
    "                if v > 0.6:\n",
    "                    num.set_color(WHITE)\n",
    "                else:\n",
    "                    num.set_color(BLACK)\n",
    "\n",
    "                cells.add(VGroup(sq, num))\n",
    "\n",
    "        self.play(FadeIn(cells), run_time=1.0)\n",
    "\n",
    "        # --- Colorbar legend on the right ----------------------------------\n",
    "        # simple vertical gradient bar + ticks 0, 0.5, 1\n",
    "        bar_height = seq_len * (cell_size + 0.05)\n",
    "        bar_width = 0.3\n",
    "\n",
    "        # anchor bar to right of grid\n",
    "        bar_x = (seq_len/2) * (cell_size + 0.05) + 1.0\n",
    "        bar_center = np.array([bar_x, 0, 0])\n",
    "\n",
    "        # build bar as multiple thin rectangles\n",
    "        n_steps = 20\n",
    "        bar_rects = VGroup()\n",
    "        for k in range(n_steps):\n",
    "            frac = k / (n_steps - 1)\n",
    "            r = Rectangle(\n",
    "                height=bar_height / n_steps,\n",
    "                width=bar_width,\n",
    "            )\n",
    "            r.set_fill(interpolate_color(WHITE, BLUE, frac), opacity=0.9)\n",
    "            r.set_stroke(width=0)\n",
    "            y = (frac - 0.5) * bar_height\n",
    "            r.move_to(bar_center + np.array([0, y, 0]))\n",
    "            bar_rects.add(r)\n",
    "\n",
    "        # ticks & labels\n",
    "        ticks = VGroup()\n",
    "        labels = VGroup()\n",
    "        for val, frac in [(0.0, 0.0), (0.5, 0.5), (1.0, 1.0)]:\n",
    "            y = (frac - 0.5) * bar_height\n",
    "            tick = Line(\n",
    "                bar_center + np.array([-bar_width/2, y, 0]),\n",
    "                bar_center + np.array([-bar_width/2 - 0.1, y, 0]),\n",
    "                stroke_width=2\n",
    "            )\n",
    "            lbl = Text(f\"{val:.1f}\", font_size=20)\n",
    "            lbl.next_to(tick, LEFT, buff=0.1)\n",
    "            ticks.add(tick)\n",
    "            labels.add(lbl)\n",
    "\n",
    "        colorbar_group = VGroup(bar_rects, ticks, labels)\n",
    "        self.play(FadeIn(colorbar_group), run_time=0.8)\n",
    "\n",
    "        # --- Optional: subtitle with sentence ------------------------------\n",
    "        if text:\n",
    "            subtitle = Text(text, font_size=24).next_to(title, DOWN, buff=0.3)\n",
    "            self.play(Write(subtitle), run_time=0.8)\n",
    "\n",
    "        self.wait(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64356a",
   "metadata": {},
   "source": [
    "The Manim Jupyter extension (manim.utils.ipython_magic) takes the rendered video and inserts it directly into the output cell using HTML <video> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c027fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -ql -v WARNING TokenGraphScene\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "\n",
    "class TokenGraphScene(Scene):\n",
    "    def construct(self):\n",
    "        # Load single-layer/head attention data\n",
    "        data = np.load(\"attention_data.npz\", allow_pickle=True)\n",
    "        raw_tokens = data[\"tokens\"]\n",
    "        tokens = [str(t) for t in raw_tokens.tolist()]\n",
    "        matrix = data[\"matrix\"]\n",
    "        layer_idx = int(data.get(\"layer_idx\", -1))\n",
    "        head_idx = int(data.get(\"head_idx\", -1))\n",
    "        text = str(data.get(\"text\", \"\"))\n",
    "\n",
    "        seq_len = len(tokens)\n",
    "\n",
    "        # Title\n",
    "        title = Text(f\"Token Graph — Layer {layer_idx}, Head {head_idx}\", font_size=36)\n",
    "        title.to_edge(UP)\n",
    "        self.add(title)\n",
    "\n",
    "        if text:\n",
    "            subtitle = Text(text, font_size=24).next_to(title, DOWN, buff=0.3)\n",
    "            self.play(Write(subtitle), run_time=0.8)\n",
    "\n",
    "        # Place tokens on a circle\n",
    "        radius = 3.0\n",
    "        angle_step = TAU / seq_len\n",
    "        token_nodes = []\n",
    "        for i, tok in enumerate(tokens):\n",
    "            angle = i * angle_step\n",
    "            pos = np.array([radius * np.cos(angle), radius * np.sin(angle), 0])\n",
    "            label = Text(tok, font_size=24).move_to(pos)\n",
    "            token_nodes.append(label)\n",
    "\n",
    "        self.play(*[FadeIn(node, shift=0.2*OUT) for node in token_nodes], run_time=1.2)\n",
    "\n",
    "        # Build list of edges (i -> j) sorted by attention weight\n",
    "        edges = []\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                w = float(matrix[i, j])\n",
    "                if w > 0.05:  # small threshold to avoid clutter\n",
    "                    edges.append((i, j, w))\n",
    "        edges.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # Animate edges from strongest to weakest\n",
    "        edge_mobs = []\n",
    "        max_w = max([e[2] for e in edges]) if edges else 1.0\n",
    "\n",
    "        for i, j, w in edges:\n",
    "            start = token_nodes[i].get_center()\n",
    "            end = token_nodes[j].get_center()\n",
    "            alpha = w / (max_w + 1e-8)\n",
    "\n",
    "            # curved arrow for aesthetics\n",
    "            arrow = CurvedArrow(\n",
    "                start,\n",
    "                end,\n",
    "                angle=0.3,\n",
    "                stroke_width=2 + 5 * alpha,\n",
    "                color=interpolate_color(GRAY, BLUE, alpha),\n",
    "                tip_length=0.2,\n",
    "            )\n",
    "            edge_mobs.append(arrow)\n",
    "            self.play(Create(arrow), run_time=0.1)\n",
    "\n",
    "        self.wait(2)\n",
    "        self.play(*[FadeOut(m) for m in edge_mobs + token_nodes], run_time=1.0)\n",
    "        self.wait(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c839732",
   "metadata": {},
   "source": [
    "Bertsviz already has great token graphs, but we can use manim if we want to animate it, make it look better and more customized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d002c3b6",
   "metadata": {},
   "source": [
    "# Embedding Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "packages = [\n",
    "    \"manim\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"umap-learn\",\n",
    "    \"scikit-learn\",\n",
    "]\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *packages])\n",
    "print(\"Dependencies installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0705dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter config\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Load manim IPython magic\n",
    "%load_ext manim\n",
    "\n",
    "print(\"Libraries imported and manim extension loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n",
    "model = AutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of layers: {config.n_layers}\")\n",
    "print(f\"Hidden size: {config.dim}\")\n",
    "print(f\"Number of attention heads per layer: {config.n_heads}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layerwise_embeddings(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract embeddings from all layers for a given text.\n",
    "    \n",
    "    Returns:\n",
    "        tokens: list of token strings\n",
    "        layer_embeddings: list of (seq_len, hidden_dim) arrays\n",
    "        input_ids: token ids tensor\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    print(\"inputs\", inputs)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # outputs.hidden_states: (embedding_output, layer_1, ..., layer_n)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    layer_embeddings = [h[0].cpu().numpy() for h in hidden_states]  # drop batch dim\n",
    "\n",
    "    return tokens, layer_embeddings, inputs[\"input_ids\"]\n",
    "\n",
    "# quick sanity check\n",
    "sample_text = \"I will present the present at the meeting.\"\n",
    "tokens_test, layer_emb_test, _ = extract_layerwise_embeddings(sample_text, model, tokenizer)\n",
    "print(\"Tokens:\", tokens_test)\n",
    "print(\"Num layers (incl. embedding layer 0):\", len(layer_emb_test))\n",
    "print(\"Shape per layer:\", layer_emb_test[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(embeddings, method='umap', n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Reduce embedding dimensionality using UMAP, t-SNE, or PCA.\n",
    "    \"\"\"\n",
    "    if method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state, n_neighbors=15)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=n_components, random_state=random_state, perplexity=30)\n",
    "    elif method == 'pca':\n",
    "        reducer = PCA(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def prepare_embeddings_for_reduction(layer_embeddings, exclude_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Stack embeddings from all layers into a single array for DR.\n",
    "\n",
    "    Returns:\n",
    "        all_embeddings: (N, hidden_dim)\n",
    "        layer_labels:   (N,)  -> which layer it came from\n",
    "        token_positions:(N,)  -> which token index it came from\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    layer_labels = []\n",
    "    token_positions = []\n",
    "\n",
    "    for layer_idx, embeddings in enumerate(layer_embeddings):\n",
    "        seq_len = embeddings.shape[0]\n",
    "\n",
    "        if exclude_special_tokens:\n",
    "            start_idx = 1\n",
    "            end_idx = seq_len - 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "            end_idx = seq_len\n",
    "\n",
    "        for pos in range(start_idx, end_idx):\n",
    "            all_embeddings.append(embeddings[pos])\n",
    "            layer_labels.append(layer_idx)\n",
    "            token_positions.append(pos)\n",
    "\n",
    "    return np.array(all_embeddings), np.array(layer_labels), np.array(token_positions)\n",
    "\n",
    "print(\"Helper functions ready: reduce_dimensions, prepare_embeddings_for_reduction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def save_two_token_evolution_3d_for_manim(\n",
    "    sentence,\n",
    "    token_of_interest,\n",
    "    filename=\"two_token_evolution_3d.npz\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute layer-wise embeddings for TWO occurrences of a token,\n",
    "    run PCA→3D jointly, and save everything Manim needs.\n",
    "\n",
    "    - sentence: the input text\n",
    "    - token_of_interest: e.g. \"present\"\n",
    "    - filename: npz file to save\n",
    "    \"\"\"\n",
    "    tokens, layer_embeddings, _ = extract_layerwise_embeddings(sentence, model, tokenizer)\n",
    "\n",
    "    # Find all indices of the token in the tokenized sequence\n",
    "    indices = [i for i, t in enumerate(tokens) if t == token_of_interest]\n",
    "    if len(indices) < 2:\n",
    "        raise ValueError(\n",
    "            f\"Need at least 2 occurrences of '{token_of_interest}' in tokens, \"\n",
    "            f\"but found {len(indices)}.\\nTokens: {tokens}\"\n",
    "        )\n",
    "\n",
    "    idx1, idx2 = indices[0], indices[1]\n",
    "\n",
    "    # Stack embeddings across layers for each occurrence\n",
    "    # token_evolution_1: (num_layers, hidden_dim)\n",
    "    # token_evolution_2: (num_layers, hidden_dim)\n",
    "    token_evolution_1 = np.array([layer[idx1] for layer in layer_embeddings])\n",
    "    token_evolution_2 = np.array([layer[idx2] for layer in layer_embeddings])\n",
    "\n",
    "    num_layers, hidden_dim = token_evolution_1.shape\n",
    "\n",
    "    # PCA → 3D, fit jointly on both trajectories so they share the same 3D space\n",
    "    stacked = np.vstack([token_evolution_1, token_evolution_2])  # (2*num_layers, hidden_dim)\n",
    "    pca = PCA(n_components=3)\n",
    "    stacked_3d = pca.fit_transform(stacked)  # (2*num_layers, 3)\n",
    "\n",
    "    coords1_3d = stacked_3d[:num_layers]       # (num_layers, 3)\n",
    "    coords2_3d = stacked_3d[num_layers:]       # (num_layers, 3)\n",
    "\n",
    "    np.savez(\n",
    "        filename,\n",
    "        coords1=coords1_3d,\n",
    "        coords2=coords2_3d,\n",
    "        sentence=sentence,\n",
    "        token=token_of_interest,\n",
    "        idx1=idx1,\n",
    "        idx2=idx2,\n",
    "        num_layers=num_layers,\n",
    "        tokens=np.array(tokens),\n",
    "    )\n",
    "    print(f\"Saved 3D evolution for two '{token_of_interest}' occurrences to {filename}\")\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(f\"Occurrences used: indices {idx1} and {idx2}\")\n",
    "\n",
    "# EXAMPLE: your sentence with two 'present's\n",
    "sentence = \"I will present the present at the meeting.\"\n",
    "save_two_token_evolution_3d_for_manim(sentence, \"present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc04697",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -ql -v WARNING TokenEvolutionScene\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "\n",
    "class TokenEvolutionScene(Scene):\n",
    "    def construct(self):\n",
    "        data = np.load(\"token_evolution.npz\", allow_pickle=True)\n",
    "\n",
    "        points = data[\"points\"]               # (num_layers, 2)\n",
    "        sentence = str(data[\"sentence\"])\n",
    "        token = str(data[\"token\"])\n",
    "        occurrence = int(data[\"occurrence\"])\n",
    "        num_layers = int(data[\"num_layers\"])\n",
    "\n",
    "        # Normalize coordinates for nicer layout\n",
    "        xs = points[:, 0]\n",
    "        ys = points[:, 1]\n",
    "        max_range = max(xs.max() - xs.min(), ys.max() - ys.min()) + 1e-6\n",
    "        points_norm = np.column_stack([\n",
    "            (xs - xs.mean()) / max_range * 8,\n",
    "            (ys - ys.mean()) / max_range * 4,\n",
    "        ])\n",
    "\n",
    "        # Title & subtitle\n",
    "        title = Text(f\"Token evolution across layers: '{token}'\", font_size=36)\n",
    "        title.to_edge(UP)\n",
    "        self.add(title)\n",
    "\n",
    "        subtitle = Text(\n",
    "            f\"Sentence: {sentence}  (occurrence {occurrence})\",\n",
    "            font_size=24\n",
    "        ).next_to(title, DOWN, buff=0.3)\n",
    "        self.play(Write(subtitle), run_time=0.8)\n",
    "\n",
    "        # Axes\n",
    "        axes = Axes(\n",
    "            x_range=[-5, 5, 1],\n",
    "            y_range=[-3, 3, 1],\n",
    "            x_length=10,\n",
    "            y_length=6,\n",
    "            tips=False\n",
    "        )\n",
    "        axes.move_to(ORIGIN)\n",
    "        self.play(Create(axes), run_time=0.7)\n",
    "\n",
    "        # All layer points as faint background\n",
    "        all_dots = VGroup()\n",
    "        for i, (x, y) in enumerate(points_norm):\n",
    "            dot = Dot(axes.c2p(x, y), radius=0.06, color=GRAY)\n",
    "            label = Text(f\"L{i}\", font_size=18).next_to(dot, UP, buff=0.1)\n",
    "            all_dots.add(VGroup(dot, label))\n",
    "        self.play(FadeIn(all_dots, run_time=0.5))\n",
    "\n",
    "        # Moving highlighted dot\n",
    "        current_dot = Dot(axes.c2p(*points_norm[0]), radius=0.12, color=YELLOW)\n",
    "        layer_label = Text(\"Layer 0\", font_size=28).to_corner(UL).shift(DOWN*0.3)\n",
    "        self.play(FadeIn(current_dot), FadeIn(layer_label), run_time=0.7)\n",
    "\n",
    "        # Path object\n",
    "        path = VMobject(stroke_color=YELLOW, stroke_width=4)\n",
    "        path.set_points_as_corners([axes.c2p(*points_norm[0])])\n",
    "        self.add(path)\n",
    "\n",
    "        for layer_idx in range(1, num_layers):\n",
    "            new_point = axes.c2p(*points_norm[layer_idx])\n",
    "            new_layer_label = Text(f\"Layer {layer_idx}\", font_size=28).to_corner(UL).shift(DOWN*0.3)\n",
    "\n",
    "            self.play(\n",
    "                current_dot.animate.move_to(new_point),\n",
    "                ReplacementTransform(layer_label, new_layer_label),\n",
    "                run_time=0.8,\n",
    "            )\n",
    "            layer_label = new_layer_label\n",
    "\n",
    "        self.wait(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e619af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -qk -v WARNING TwoTokenEvolution3DVectorsScene\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "\n",
    "class TwoTokenEvolution3DVectorsScene(ThreeDScene):\n",
    "    def construct(self):\n",
    "        data = np.load(\"two_token_evolution_3d.npz\", allow_pickle=True)\n",
    "\n",
    "        coords1 = data[\"coords1\"]    # (num_layers, 3)\n",
    "        coords2 = data[\"coords2\"]    # (num_layers, 3)\n",
    "        num_layers = int(data[\"num_layers\"])\n",
    "        token = str(data[\"token\"])\n",
    "        sentence = str(data[\"sentence\"])\n",
    "\n",
    "        # -------- Normalize coords for nicer view --------\n",
    "        all_coords = np.vstack([coords1, coords2])\n",
    "        max_abs = np.max(np.abs(all_coords)) + 1e-6\n",
    "        scale = 4 / max_abs\n",
    "        coords1 = coords1 * scale\n",
    "        coords2 = coords2 * scale\n",
    "\n",
    "        # -------- Titles (fixed in frame) --------\n",
    "        title = Text(\n",
    "            f\"3D Vector Evolution of Two '{token}' Occurrences\",\n",
    "            font_size=32\n",
    "        ).to_edge(UP)\n",
    "        subtitle = Text(\n",
    "            f\"Sentence: {sentence}\",\n",
    "            font_size=20\n",
    "        ).next_to(title, DOWN, buff=0.2)\n",
    "\n",
    "        self.add_fixed_in_frame_mobjects(title, subtitle)\n",
    "\n",
    "        # -------- Camera + axes --------\n",
    "        self.set_camera_orientation(phi=65 * DEGREES, theta=30 * DEGREES)\n",
    "        self.begin_ambient_camera_rotation(rate=0.10)\n",
    "\n",
    "        axes = ThreeDAxes(\n",
    "            x_range=[-4, 4, 2],\n",
    "            y_range=[-4, 4, 2],\n",
    "            z_range=[-4, 4, 2],\n",
    "            x_length=8,\n",
    "            y_length=8,\n",
    "            z_length=8,\n",
    "        )\n",
    "        self.play(Create(axes), run_time=1.5)\n",
    "\n",
    "        origin = axes.c2p(0, 0, 0)\n",
    "\n",
    "        # -------- Layer tracker --------\n",
    "        layer_tracker = ValueTracker(0)\n",
    "\n",
    "        # -------- Arrows = token vectors from origin --------\n",
    "        vector1 = always_redraw(\n",
    "            lambda: Arrow3D(\n",
    "                start=origin,\n",
    "                end=axes.c2p(*coords1[int(layer_tracker.get_value())]),\n",
    "                color=BLUE,\n",
    "                thickness=0.03,\n",
    "            )\n",
    "        )\n",
    "        vector2 = always_redraw(\n",
    "            lambda: Arrow3D(\n",
    "                start=origin,\n",
    "                end=axes.c2p(*coords2[int(layer_tracker.get_value())]),\n",
    "                color=RED,\n",
    "                thickness=0.03,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Optional: spheres at the tips\n",
    "        tip1 = always_redraw(\n",
    "            lambda: Dot3D(\n",
    "                axes.c2p(*coords1[int(layer_tracker.get_value())]),\n",
    "                radius=0.08,\n",
    "                color=BLUE,\n",
    "            )\n",
    "        )\n",
    "        tip2 = always_redraw(\n",
    "            lambda: Dot3D(\n",
    "                axes.c2p(*coords2[int(layer_tracker.get_value())]),\n",
    "                radius=0.08,\n",
    "                color=RED,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.play(FadeIn(vector1), FadeIn(vector2), FadeIn(tip1), FadeIn(tip2), run_time=1.0)\n",
    "\n",
    "        # -------- Layer label (screen-fixed) --------\n",
    "        layer_label = always_redraw(\n",
    "            lambda: Text(\n",
    "                f\"Layer {int(layer_tracker.get_value())}\",\n",
    "                font_size=26\n",
    "            ).to_corner(UL).shift(DOWN * 0.3)\n",
    "        )\n",
    "        self.add_fixed_in_frame_mobjects(layer_label)\n",
    "\n",
    "        # -------- Animate layer-by-layer --------\n",
    "        for i in range(1, num_layers):\n",
    "            self.play(\n",
    "                layer_tracker.animate.set_value(i),\n",
    "                run_time=0.8,\n",
    "            )\n",
    "\n",
    "        self.wait(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_layer_umap_for_manim(\n",
    "    sentence,\n",
    "    filename=\"layer_umap_for_manim.npz\",\n",
    "    exclude_special_tokens=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute layer-wise embeddings for a sentence, UMAP→2D for all\n",
    "    (layer, token) embeddings, reshape to [num_layers, num_tokens, 2]\n",
    "    for animation.\n",
    "    \"\"\"\n",
    "    tokens, layer_embeddings, _ = extract_layerwise_embeddings(sentence, model, tokenizer)\n",
    "\n",
    "    all_embeddings, layer_labels, token_positions = prepare_embeddings_for_reduction(\n",
    "        layer_embeddings,\n",
    "        exclude_special_tokens=exclude_special_tokens,\n",
    "    )\n",
    "\n",
    "    embeddings_2d = reduce_dimensions(all_embeddings, method=\"umap\", n_components=2)\n",
    "\n",
    "    num_layers = len(layer_embeddings)\n",
    "    seq_len = layer_embeddings[0].shape[0]\n",
    "\n",
    "    if exclude_special_tokens:\n",
    "        start_idx = 1\n",
    "        end_idx = seq_len - 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        end_idx = seq_len\n",
    "\n",
    "    num_tokens = end_idx - start_idx\n",
    "\n",
    "    # Reshape: [num_layers, num_tokens, 2]\n",
    "    coords = embeddings_2d.reshape(num_layers, num_tokens, 2)\n",
    "    tokens_no_special = tokens[start_idx:end_idx]\n",
    "\n",
    "    np.savez(\n",
    "        filename,\n",
    "        coords=coords,\n",
    "        tokens=tokens_no_special,\n",
    "        num_layers=num_layers,\n",
    "        num_tokens=num_tokens,\n",
    "        sentence=sentence,\n",
    "    )\n",
    "    print(f\"Saved layer-wise UMAP coords to {filename}\")\n",
    "    print(f\"Num layers: {num_layers}, num tokens (no specials): {num_tokens}\")\n",
    "    print(\"Tokens (no specials):\", tokens_no_special)\n",
    "\n",
    "# EXAMPLE: same sentence\n",
    "sentence = \"I will present the present at the meeting.\"\n",
    "save_layer_umap_for_manim(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa04198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%manim -ql -v WARNING LayerwiseEmbeddingsScene\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "\n",
    "class LayerwiseEmbeddingsScene(Scene):\n",
    "    def construct(self):\n",
    "        data = np.load(\"layer_umap_for_manim.npz\", allow_pickle=True)\n",
    "        coords = data[\"coords\"]          # (num_layers, num_tokens, 2)\n",
    "        tokens = [str(t) for t in data[\"tokens\"].tolist()]\n",
    "        num_layers = int(data[\"num_layers\"])\n",
    "        num_tokens = int(data[\"num_tokens\"])\n",
    "        sentence = str(data[\"sentence\"])\n",
    "\n",
    "        # Normalize coords\n",
    "        xs = coords[..., 0].flatten()\n",
    "        ys = coords[..., 1].flatten()\n",
    "        max_range = max(xs.max() - xs.min(), ys.max() - ys.min()) + 1e-6\n",
    "        coords_norm = coords.copy()\n",
    "        coords_norm[..., 0] = (coords[..., 0] - xs.mean()) / max_range * 8\n",
    "        coords_norm[..., 1] = (coords[..., 1] - ys.mean()) / max_range * 4\n",
    "\n",
    "        # Title + subtitle\n",
    "        title = Text(\"Layer-wise token embeddings (UMAP)\", font_size=36)\n",
    "        title.to_edge(UP)\n",
    "        self.add(title)\n",
    "\n",
    "        subtitle = Text(\n",
    "            f\"Sentence: {sentence}\",\n",
    "            font_size=24\n",
    "        ).next_to(title, DOWN, buff=0.3)\n",
    "        self.play(Write(subtitle), run_time=0.8)\n",
    "\n",
    "        # Axes\n",
    "        axes = Axes(\n",
    "            x_range=[-5, 5, 1],\n",
    "            y_range=[-3, 3, 1],\n",
    "            x_length=10,\n",
    "            y_length=6,\n",
    "            tips=False\n",
    "        )\n",
    "        axes.move_to(ORIGIN)\n",
    "        self.play(Create(axes), run_time=0.7)\n",
    "\n",
    "        # Initial positions (layer 0)\n",
    "        dots = VGroup()\n",
    "        labels = VGroup()\n",
    "        for i in range(num_tokens):\n",
    "            x, y = coords_norm[0, i]\n",
    "            dot = Dot(axes.c2p(x, y), radius=0.09, color=BLUE)\n",
    "            label = Text(tokens[i], font_size=18).next_to(dot, UP, buff=0.1)\n",
    "            dots.add(dot)\n",
    "            labels.add(label)\n",
    "\n",
    "        layer_label = Text(\"Layer 0\", font_size=28).to_corner(UL).shift(DOWN*0.3)\n",
    "        self.play(FadeIn(dots), FadeIn(labels), FadeIn(layer_label), run_time=0.8)\n",
    "\n",
    "        # Animate movement through layers\n",
    "        for layer_idx in range(1, num_layers):\n",
    "            new_positions = coords_norm[layer_idx]\n",
    "            new_layer_label = Text(f\"Layer {layer_idx}\", font_size=28).to_corner(UL).shift(DOWN*0.3)\n",
    "\n",
    "            self.play(\n",
    "                *[\n",
    "                    dot.animate.move_to(axes.c2p(*new_positions[i]))\n",
    "                    for i, dot in enumerate(dots)\n",
    "                ],\n",
    "                ReplacementTransform(layer_label, new_layer_label),\n",
    "                run_time=0.9,\n",
    "            )\n",
    "            layer_label = new_layer_label\n",
    "            self.wait(0.3)\n",
    "\n",
    "        self.wait(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glassbox-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
